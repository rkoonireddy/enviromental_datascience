---
title: "Week 3 Tutorial: Exploratory Data Analysis"
author: "Eliza Harris"
date: "2023-10-03"
output: html_document
---

In this tutorial and the following exercise, we will deal with some more complex datasets and work through the data exploration skills we have learnt about in the lecture videos. We will create functions to make our code more flexible and reusable.

## Workspace

We will begin by loading the packages we need for the code in this tutorial. If you are not familiar with any of the packages, search the package documentation to find out what we are using it for.

```{r message=FALSE}
require(lubridate)
require(tidyverse)
require(ncdf4)
require(viridis)
require(raster)
require(rworldmap)
require(EFDR)
library(RColorBrewer)
```

## Data profiling

We will start by importing global mean temperature data, part of the HadCRUT reanalysis dataset produced by the UK Met Office and the Climate Research Unit at the University of East Anglia (https://crudata.uea.ac.uk/cru/data/temperature/).

This data is the mean monthly temperature per gridcell from 1961-1990, and is used as a baseline to calculate temperature anomalies driven by climate change. The data are in netcdf format (.nc), which is useful for multidimensional datasets, and helps us store both data and metadata. Netcdf is often used in climate and atmospheric research to deal with the four key dimensions of time, latitude, longitude, and altitude.

```{r}
nc_data = nc_open('./data/absolute_v5.nc') # Open the file to read the netcdf data
print(nc_data) # Runs an "nc dump" to show a summary of the file
```

The nc dump shows us an overview of the file. At the bottom of the dump we can see 8 global attributes; attributes belonging to the dataset as a whole. These describe the history, ownership and license of the data. Netcdf is a widely used format because of the ability to save metadata as attributes, and to keep multiple dimensions of data in a single file. R, python and other data analysis programs have packages to read netcdf; here we have required the **ncdf4** package.

At the start of the dump, we see the variables contained in the file. This file has only one variable; temperature (**tem**), with dimensions of [lon,lat,time]. The **tem** variable has **attributes**, which we can see include the full name of the variable, the units, offset and scaling factors, and fill and missing values. Netcdf efficiently allows us to combine metadata and multidimensional data. 

After the variables we see the dimensions. There are 3 dimensions, **time**, **lon** and **lat**. We can see the attributes of the dimensions, including full names, units, axes. Longitude and latitude have sizes of 72 and 36 respectively, corresponding to 5&deg; grid cells. If we look back at **tem** we see it is defined in terms of **[lon,lat,time]**, which means that temperature is a 3D variable following these 3 defined dimensions - the size of **tem** must equal these three dimensions. The variable is surface temperature so we don't have the fourth dimension of altitude or atmospheric level. 

## Data cleaning

Here we will select the variables we need from the netcdf data.
```{r}
lon <- ncvar_get(nc_data, "lon")
head(lon) # show the start of the lon data vector

lat <- ncvar_get(nc_data, "lat", verbose = F)
head(lat) # show the start of the lat data vector

tem <- ncvar_get(nc_data, "tem")
dim(tem) # temperature has dimensions of lon,lat,month

# Get also the missing value designator (we know this is -9999 from the nc dump)
fillvalue <- ncatt_get(nc_data, "tem", "missing_value", verbose="F") # This functions allows us to retrieve an attribute called "missing_value" from the "tem" variable in the nc_data dataset
fillvalue 

nc_close(nc_data) # We have the data so we can close the netcdf file
```

We will look for missing values and replace with a standard NaN.

```{r}
sum(tem == fillvalue$value) # See how many missing values there are
tem[tem == fillvalue$value] = NaN
# This is a cleaned reanalysis dataset so there are no missing values, but we run the code as an illustration
```

Finally, we will save our temperature dataset as a list to keep our workspace tidy.

```{r}
abs_tem <- list(lat=lat,lon=lon,tem=tem)
```

## Data reduction

To illustrate data reduction procedures, we will look at the mean monthly temperature in different latitude bands. First we will use **discretization** to select several latitude bands, and then we will **aggregate** to find the monthly means for each of these bands.

```{r}
lat_cutoffs <- c(-90,-60,-30,0,30,60,90)

# Preallocate an array for the results; dimensions = time (month), latitude band, mean/std
mean_tem_latbands <- array(dim=c(12,length(lat_cutoffs)-1,2))

# Discretize latitude
abs_tem$lat_band <- abs_tem$tem*NaN # space for discretized lat band, same size as tem 
for (n in 1:(length(lat_cutoffs)-1)){ # Loop through the lat bands
  # Find lats corresponding to this band
  tmp = abs_tem$lat > lat_cutoffs[n] & abs_tem$lat <= lat_cutoffs[n+1]
  # Give a number to each band
  abs_tem$lat_band[,tmp,] <- n 
}

# Monthly mean for each band
for (n in 1:12){
  # convert tem data and lat band labels to vector so we can use aggregate
  labels <- as.vector(abs_tem$lat_band[,,n]) 
  data <- as.vector(abs_tem$tem[,,n])
  mean_tem_latbands[n,,1] <- aggregate(data, list(labels), FUN=mean)$x
  mean_tem_latbands[n,,2] <- aggregate(data, list(labels), FUN=sd)$x
}

# Look at the results: mean and std are printed as separate tables, with dimensions of month * lat band
mean_tem_latbands
```

Now we will plot the mean and standard deviation of temperature for each latitude band.

```{r}
par(mar=c(2,4,1,1)) # set plotting margins
par(mfrow=c(2,1)) # Create subplots for mean and stdev or you can combine the two lines par(mar=c(2,4,1,1),mfrow=c(2,1))
colours <- viridis(6) # Viridis gives us a set of a chosen number of colour-blind friendly colours for plotting

# Plot the mean temperature
plot(1:12,mean_tem_latbands[,1,1],type="n",xlab="month",ylab="Mean T",ylim=c(-40,40),xlim=c(1,12)) # initialize empty plot 
for (n in 1:6){
  lines(1:12,mean_tem_latbands[,n,1],col=colours[n],lw=2)
}

# Plot the std in temperature
plot(1:12,mean_tem_latbands[,1,2],type="n",xlab="month",ylab="Stdev in T",ylim=c(0,25),xlim=c(1,12)) # initialize empty plot 
for (n in 1:6){
  lines(1:12,mean_tem_latbands[,n,2],col=colours[n],lw=2)
}

# Add a legend
legendtext <- paste0(head(lat_cutoffs,-1)," to ",tail(lat_cutoffs,-1))
legend("bottomleft",legend=legendtext,col=colours,lty=1,cex=0.5,lw=2)
```

From this plots, we can clearly see the seasonal and latitudinal changes in mean temperature. The second subplot shows that the lowest latitude band has much higher variability in temperature within the band than other bands; if we were working further with this data we might want to break this band up into smaller chunks.

## Data transformation: Plotting as a raster

One way to easily plot a global map is to convert "slices" of our data to rasters.

```{r}
# Convert the January slice to a raster
r <- raster(t(abs_tem$tem[,,1]), xmn=min(abs_tem$lon), xmx=max(abs_tem$lon), ymn=min(abs_tem$lat), ymx=max(abs_tem$lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
r <- flip(r, direction='y') # We need to flip upside down as southern data is first in the file

# Now we can directly use the raster with the base R plot function
plot(r,ylim=c(-90,90),xlim=c(-180,180),main="Mean January temperature")
plot(coastsCoarse,add=TRUE,col='black') # Add continental outlines from the rworldmap package

# We can use R color brewer to have more control over the color map, for example to have a blue - red scale for cold - warm temperatures. We need to give the brewer.pal function a number of discrete colours to use as well as a colour map (https://r-graph-gallery.com/38-rcolorbrewers-palettes.html). The chosen RdBu map goes from red to blue, but we want to go from blue (for smallest values) to red, so we need to manually reverse the direction. 
cmap <- rev(brewer.pal(10,"RdBu")) # create the colour map, and use rev to reverse the array
plot(r,ylim=c(-90,90),xlim=c(-180,180),main="Mean January temperature",col=cmap)
plot(coastsCoarse,add=TRUE,col='black') 
```

We might want to plot the data again, so we'll create a function to simplify this. The function takes lon, lat and data (tem) as main arguments, and the xlimits, ylimits, colour map and title can be specified as optional arguments. We'll create a few different plots to illustrate the use of the function. 

```{r}
plot_map <- function(lon,lat,data,
                    ylim=c(-90,90),xlim=c(-180,180), # Default is the whole globe
                    cmap=rev(brewer.pal(10,"RdBu")), # Default colour scale
                    title="Plot" # Default plot title
                    ){
  
  r <- raster(t(data), xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r <- flip(r, direction='y') 
  plot(r,ylim=ylim,xlim=xlim,main=title,col=cmap)
  plot(coastsCoarse,add=TRUE,col='black') 
}

plot_map(lon=abs_tem$lon,lat=abs_tem$lat,data=abs_tem$tem[,,12],
    title="Mean December temperature")

plot_map(lon=abs_tem$lon,lat=abs_tem$lat,data=abs_tem$tem[,,12],
    ylim=c(-50,50),xlim=c(-100,100),title="Mean December temperature, small grid")
```

## Data transformation: Regridding

Another common transformation is to regrid the data. The data is currently on a 5&deg; grid with 36 latitude and 72 longitude values. To regrid, we first need to create a dataframe with values x, y, z representing lon, lat, tem.

```{r}
lon_vec <- rep(abs_tem$lon,length(abs_tem$lat))
lat_vec <- rep(abs_tem$lat,each=length(abs_tem$lon))
tem_vec <- as.vector(abs_tem$tem[,,1]) # Regrid the January slice
df <- data.frame(x=lat_vec,y=lon_vec,z=tem_vec)
head(df)
```

Now we regrid the data using the **regrid** function from the EFDR package. We'll regrid to 72 latitude (previously 36) and 144 longitude (previously 72) bands, so that we can see a large difference.

```{r}
df_regrid <- regrid(
  df, # the dataframe with x, y, z to regrid
  n1 = 72, # the new x dimension
  n2 = 144, # the new y dimension
  method = "idw", # regrid method (inverse-distance-weighting)
  idp = 0.5, # the inverse distance power for weighting, eg. the weight further-away grid cells contribute with
  nmax = 4 # the max number of neighbours to use; more neighbours = more smoothing
)
```

It's useful to convert back to our original data format, ie. a list.

```{r}
lat_r <- unique(df_regrid$x)
lon_r <- unique(df_regrid$y)
tem_r <- matrix(nrow=length(lon_r),ncol=length(lat_r))
for (n in 1:length(lon_r)){ # Convert the regridded temp vector to a matrix
  istart <- (n-1)*length(lat_r)+1
  iend <- n*length(lat_r)
  tem_r[n,] <- df_regrid$z[istart:iend]
}
abs_tem_r <- list(lon=lon_r,lat=lat_r,tem=tem_r)
# This dataframe has the new gridding but the same format as the previous matrix, except that we do not have multiple months as we only regridded January.
```

Finally, plot to compare.

```{r}
par(mar=c(2,4,1,1),mfrow=c(2,1)) # set plotting parameters
plot_map(lon=abs_tem$lon,lat=abs_tem$lat,data=abs_tem$tem[,,1],
    title="Original: Mean January temperature")
plot_map(lon=abs_tem$lon,lat=abs_tem$lat,data=abs_tem_r$tem,
    title="Regridded: Mean January temperature")
```

## Comparing sites

Now we will select four grid cells at random using the *sample*, and compare them with a few of the methods we heard about in the lecture. 

```{r}
set.seed(20)
lat_i <- sample(1:36,4,replace=TRUE) # lat indexes of chosen cells
set.seed(20)
lon_i <- sample(1:72,4,replace=TRUE) # lon indexes of chosen cells
# Print the lats and lons chosen
print(paste0("lat=",abs_tem$lat[lat_i]))
print(paste0("lon=",abs_tem$lon[lon_i]))
```

We'll get the data from the gridcells into a matrix. **Site** and **Month** will be columns so that we can use them as factors for aggregation and comparisons.

```{r}
site_data <- data.frame(matrix(nrow=48,ncol=3))
colnames(site_data) <- c("site","month","tem")
for (n in 1:4){
  istart = (n-1)*12+1
  site_data$site[istart:(istart+11)] = n
  site_data$month[istart:(istart+11)] = 1:12
  site_data$tem[istart:(istart+11)] = abs_tem$tem[lon_i[n],lat_i[n],]
}
site_data

# Get the annual mean and stdev at each site
aggregate(site_data$tem, list(site_data$site), FUN=mean) 
aggregate(site_data$tem, list(site_data$site), FUN=sd) 
```

Now we'll look at a few ways to compare the sites. We are not going to go into the statistics in detail - that is covered in your basic statistics courses. Here we just focus on implementation and interpretation of these techniques in R.

First, we will manually select and compare each pair of sites using two different common tests, the t test and the Mann-Whitney test, using paired and unpaired data.

```{r, warning=FALSE}
# t test, unpaired and pairwise
ttest_res <- array(dim=c(4,4,2)) # space for results
ttestpair_res <- array(dim=c(4,4,2))
# mann whitney test, unpaired and pairwise
mw_res <- matrix(nrow=4,ncol=4)
mwpair_res <- matrix(nrow=4,ncol=4)
for (n in 1:4){
  for (i in 1:4){ 
    # in this double loop, we select each of our pairs of sites in turn
    s1 <- site_data$tem[site_data$site==n]
    s2 <- site_data$tem[site_data$site==i]
    ## perform unpaired t test
    res = t.test(s1, s2, paired = FALSE, alternative = "two.sided")
    ttest_res[n,i,1] <- res$p.value # save the p value
    ttest_res[n,i,2] <- res$estimate[1]-res$estimate[2] # estimate is the difference in means
    ## perform paired t test
    res <- t.test(s1, s2, paired = TRUE, alternative = "two.sided")
    ttestpair_res[n,i,1] = res$p.value # save the p value
    ttestpair_res[n,i,2] = res$estimate # estimate is the mean difference between monthly values
    ## perform unpaired mw-wilcox test
    res <- wilcox.test(s1, s2, paired = FALSE)
    mw_res[n,i] <- res$p.value # save the p value
    ## perform paired mw-wilcox test
    res <- wilcox.test(s1, s2, paired = TRUE)
    mwpair_res[n,i] <- res$p.value # save the p value
  }
}

# Print mean diffs (t test estimates)
print(ttest_res[,,2])
print(ttestpair_res[,,2])

# Print p values for all tests
print(ttest_res[,,1])
print(ttestpair_res[,,1])
print(mw_res)
print(mwpair_res)
```

Looking at the results, we see that the differences in means (second z dimension of t-test arrays) were largest between sites 1 and 3, and smallest between sites 3 and 4, as we saw when we found the means with aggregate. The differences between means are all significant except for between sites 3 and 4, which is expected when we look at the means and standard deviations we found with aggregate. The p-values are mostly lower (= more significant) for the paired test. In the paired test, sites 3 and 4 are significantly different, as the seasonal cycle is the same for both sites. For site 1, p-values are lower for the paired test - it is the only Southern hemisphere site and has opposite seasonality.

```{r}

```

The Mann-Whitney-Wilcoxon test results show similar patterns. We would need to test for normality to know which test is giving the most appropriate results. 

These results have illustrated how to implement different tests in R, and show us the importance of choosing the right test. We need to think about whether we want to compare annual mean temperature, or temperature in paired months. We could also think about paired tests after accounting for differences in the timing of seasonality between hemispheres.

Another common test is an ANOVA, with a post-hoc test like TukeyHSD to determine adjusted p-values and confidence levels. This can be implemented for multiple pairwise comparisons using built in functions, thus requiring much less code.

```{r}
#  Perform TukeyHSD with one-way ANOVA
model <- aov(tem ~ as.factor(site), data = site_data)
summary(model)
TukeyHSD(model, conf.level=.95)
```

We see similar results: Significant differences except between sites 3 and 4. Additionally, we get an estimate of the lower and upper confidence intervals for the difference between each pair of sites.  

Finally, we will find the Euclidean distance between each pair of sites.

```{r, warning=FALSE}
# The stats distance function needs a matrix, and compares all rows of the matrix
x <- rbind(site_data$tem[site_data$site==1],
    site_data$tem[site_data$site==2],
    site_data$tem[site_data$site==3],
    site_data$tem[site_data$site==4])
print(x)

stats::dist(x, method = "euclidean")
# This also can be used with methods: "maximum", "manhattan", "canberra", "binary" or "minkowski"
```

As we found with the other methods, the difference is greatest between sites 1 and 3, and smallest between sites 3 and 4. 

In this tutorial, we have learnt:

* How to open and use data from a netcdf (.nc) file
* Basic data preprocessing skills
* Regridding and plotting geographical datasets
* Creating and using a function with optional inputs
* Implementing different statistical techniques to compare samples




